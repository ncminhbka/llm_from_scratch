import requests
from bs4 import BeautifulSoup
import time
import random

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
}

# ==============================
# 1) HÃ m táº£i HTML an toÃ n
# ==============================
def fetch(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code == 200:
            return r.text
    except:
        return None
    return None

# ==============================
# 2) Extract text tá»« HTML
# ==============================
def extract_text(html):
    soup = BeautifulSoup(html, "html.parser")

    # XoÃ¡ script & style
    for tag in soup(["script", "style", "header", "footer", "nav", "noscript"]):
        tag.extract()

    # Láº¥y toÃ n bá»™ text clean
    text = soup.get_text(separator="\n")
    # Loáº¡i bá» dÃ²ng quÃ¡ ngáº¯n
    lines = [l.strip() for l in text.split("\n") if len(l.strip()) > 40]
    return "\n".join(lines)

# ==============================
# 3) CÃ¡c trang Ä‘á»ƒ crawl
# ==============================
URLS = [
    # bÃ¡o
    "https://vnexpress.net/",
    "https://tuoitre.vn/",
    "https://dantri.com.vn/",
    "https://news.zing.vn/",
    # blog & review
    "https://www.tinhte.vn/",
    "https://vietcetera.com/vn",
    "https://genk.vn/",
    "https://cafef.vn/",
    # thÃªm vÃ i trang tiáº¿ng Viá»‡t khÃ¡c
    "https://kenh14.vn/",
    "https://cafebiz.vn/",
]

# ==============================
# 4) Crawl link tá»« trang chá»§
# ==============================
def extract_links(home_html, home_url):
    soup = BeautifulSoup(home_html, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # Bá» cÃ¡c link rÃ¡c
        if href.startswith("javascript") or href.startswith("#"):
            continue

        # Chuyá»ƒn relative â†’ absolute
        if href.startswith("/"):
            href = home_url.rstrip("/") + href

        if href.startswith("http") and home_url.split("//")[1].split("/")[0] in href:
            links.append(href)

    # Láº¥y tá»‘i Ä‘a 40 link / trang
    return list(set(links))[:40]

# ==============================
# 5) Main
# ==============================
def crawl_all(output="tiny.txt"):
    all_text = []

    for url in URLS:
        print(f"Crawl trang: {url}")

        home = fetch(url)
        if not home:
            print("  âŒ KhÃ´ng táº£i Ä‘Æ°á»£c trang")
            continue

        links = extract_links(home, url)
        print(f"  TÃ¬m Ä‘Æ°á»£c {len(links)} link bÃ i")

        for link in links:
            print(f"    -> {link}")
            html = fetch(link)
            if not html:
                continue

            text = extract_text(html)
            if len(text) < 200:
                continue

            all_text.append(text)

            # nghá»‰ random Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
            time.sleep(random.uniform(0.5, 1.5))

    print(f"\nðŸ“Œ Tá»•ng sá»‘ bÃ i láº¥y Ä‘Æ°á»£c: {len(all_text)}")
    joined = "\n\n".join(all_text)

    with open(output, "w", encoding="utf-8") as f:
        f.write(joined)

    print(f"âœ… Ghi xong vÃ o file: {output}")


# ==============================
# Run
# ==============================
if __name__ == "__main__":
    crawl_all("tiny.txt")
